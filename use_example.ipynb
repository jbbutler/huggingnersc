{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc255720-0ff0-4499-ad7c-e4ad56d0b5fa",
   "metadata": {},
   "source": [
    "# HuggingNERSCDataset API Use Example\n",
    "\n",
    "How to use the HuggingNERSCDataset API to catalog datasets on the NERSC HuggingFace organization, and link to locations in the NERSC CFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a34b0690-e88c-416b-85d4-7f75574b14c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingnersc_dataset import HuggingNERSCDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91ee401-fa04-4066-a999-89dca73f3641",
   "metadata": {},
   "source": [
    "## 1. Make a HuggingNERSCDataset object\n",
    "\n",
    "Supply an official name to give the dataset (to be displayed on webpages, notebooks, etc.) and a nickname (to be used in directory names, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742b64de-90b3-4abd-ab88-c00cbd3a85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "official_name = 'Iris'\n",
    "nickname = 'iris'\n",
    "\n",
    "hn_iris = HuggingNERSCDataset(official_name, nickname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94c34e7-9a96-4278-8264-2ac888f88855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NERSC HuggingFace Dataset Object: \n",
       "        \n",
       " -official_name: Iris \n",
       "        \n",
       " -nickname: iris\n",
       "        \n",
       " -huggingface location: https://huggingface.co/datasets/NERSC/iris/\n",
       "        \n",
       " -nersc location: /global/cfs/cdirs/dasrepo/ai_ready_datasets/iris/"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c1699-3751-4864-8134-e3943443c605",
   "metadata": {},
   "source": [
    "*Note: the above locations are not active until the directories in the local and huggingface NERSC repos are created*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096eda1e-6a8e-43da-964c-30131e641f6f",
   "metadata": {},
   "source": [
    "## 2. Make the directories in the NERSC CFS and Huggingface repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a503d1d8-e8e7-4efa-b59e-c85a62260ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/j/jbbutler/.conda/envs/das-internship/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "hn_iris.construct_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed048ed-7b8c-4c3c-8a11-8d75a10c87f4",
   "metadata": {},
   "source": [
    "Repos will now be created at both of the locations described above. However, they will be empty. Next, we add a Jupyter notebook that can load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b4d99-3a7f-4de9-a755-f644b0d2a7fd",
   "metadata": {},
   "source": [
    "## 3. Construct loader notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a146685e-ce77-4cae-a5d5-a53e1364b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "hn_iris.construct_notebook('iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a325e93-ff50-4b87-a872-3ef440968975",
   "metadata": {},
   "source": [
    "You should now have a loader notebook in the CFS repository for your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afbed0f-a4f6-4325-a7ab-f5b83085d01a",
   "metadata": {},
   "source": [
    "## 4. Fill Huggingface README\n",
    "\n",
    "Lastly, we want to populate the Huggingface README with all of our desired metadata. This will make it easier to search for datasets within the organization. This step also populates the readme with the loader code, as well as a link to the loader notebook we just created on the CFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f005680-0ff6-455f-bf70-311e3857f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_readme = {'language':'en', \n",
    "                   'filename':'iris.csv', \n",
    "                   'tags':['pandas', 'csv', 'tabular'], \n",
    "                   'official_name':'Iris',\n",
    "                   'nickname':'iris',\n",
    "                   'size_bucket': 'n<1K'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d69abc76-81e4-4655-8e44-0b790d540687",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Provided path: '/global/cfs/cdirs/dasrepo/ai_ready_datasets/README.md' is not a file on the local file system",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m hn_iris.upload_readme(metadata_readme)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/u1/j/jbbutler/ai_ready_data_internship/huggingnersc/huggingnersc_dataset.py:80\u001b[39m, in \u001b[36mHuggingNERSCDataset.upload_readme\u001b[39m\u001b[34m(self, metadata)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# upload to huggingface\u001b[39;00m\n\u001b[32m     79\u001b[39m api = hf.HfApi()\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m api.upload_file(path_or_fileobj=NERSC_PATH + \u001b[33m'\u001b[39m\u001b[33mREADME.md\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     81\u001b[39m                 path_in_repo=\u001b[33m'\u001b[39m\u001b[33mREADME.md\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     82\u001b[39m                 repo_id=HF_ORG + \u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m + \u001b[38;5;28mself\u001b[39m.nickname, repo_type=\u001b[33m'\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/das-internship/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/das-internship/lib/python3.11/site-packages/huggingface_hub/hf_api.py:1633\u001b[39m, in \u001b[36mfuture_compatible.<locals>._inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1630\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_as_future(fn, \u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1632\u001b[39m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1633\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/das-internship/lib/python3.11/site-packages/huggingface_hub/hf_api.py:4665\u001b[39m, in \u001b[36mHfApi.upload_file\u001b[39m\u001b[34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit, run_as_future)\u001b[39m\n\u001b[32m   4660\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid repo type, must be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstants.REPO_TYPES\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   4662\u001b[39m commit_message = (\n\u001b[32m   4663\u001b[39m     commit_message \u001b[38;5;28;01mif\u001b[39;00m commit_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUpload \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with huggingface_hub\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4664\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m4665\u001b[39m operation = CommitOperationAdd(\n\u001b[32m   4666\u001b[39m     path_or_fileobj=path_or_fileobj,\n\u001b[32m   4667\u001b[39m     path_in_repo=path_in_repo,\n\u001b[32m   4668\u001b[39m )\n\u001b[32m   4670\u001b[39m commit_info = \u001b[38;5;28mself\u001b[39m.create_commit(\n\u001b[32m   4671\u001b[39m     repo_id=repo_id,\n\u001b[32m   4672\u001b[39m     repo_type=repo_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4679\u001b[39m     parent_commit=parent_commit,\n\u001b[32m   4680\u001b[39m )\n\u001b[32m   4682\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m commit_info.pr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:5\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, path_in_repo, path_or_fileobj)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/das-internship/lib/python3.11/site-packages/huggingface_hub/_commit_api.py:182\u001b[39m, in \u001b[36mCommitOperationAdd.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    180\u001b[39m     path_or_fileobj = os.path.normpath(os.path.expanduser(\u001b[38;5;28mself\u001b[39m.path_or_fileobj))\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(path_or_fileobj):\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProvided path: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_fileobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is not a file on the local file system\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.path_or_fileobj, (io.BufferedIOBase, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# ^^ Inspired from: https://stackoverflow.com/questions/44584829/how-to-determine-if-file-is-opened-in-binary-or-text-mode\u001b[39;00m\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    186\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpath_or_fileobj must be either an instance of str, bytes or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m io.BufferedIOBase. If you passed a file-like object, make sure it is\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    188\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m in binary mode.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    189\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Provided path: '/global/cfs/cdirs/dasrepo/ai_ready_datasets/README.md' is not a file on the local file system"
     ]
    }
   ],
   "source": [
    "hn_iris.upload_readme(metadata_readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408237bd-ebd0-4744-9720-abe7abbd8e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "das-internship",
   "language": "python",
   "name": "das-internship"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
